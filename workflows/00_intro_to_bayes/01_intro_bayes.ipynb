{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1459fd",
   "metadata": {},
   "source": [
    "# Sources of uncertainty\n",
    "<img src=\"imgs/sources_uncertainty.jpg\" style=\"inline: center; width: 80%;\">\n",
    "\n",
    "Fig. 1. Schematic illustration of the most important sources of uncertainty in environmental systems modeling, including (1) parameter, (2) input data (also called forcing or boundary conditions), (3), initial state, (4) model structural, (5) output, and (6) calibration data uncertainty. The measurement data error is often conveniently assumed to be known, a rather optimistic approach in most practical situations. Question remains how to describe/infer properly all sources of uncertainty in a coherent and statistically adequate manner [(Vrugt, J.A., 2016)](https://doi.org/10.1016/j.envsoft.2015.08.013)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2359b6f2",
   "metadata": {},
   "source": [
    "$$\\tilde{Y}\\leftarrow \\mathbb{R}(X^*)+\\varepsilon$$\n",
    "\n",
    "$$\\tilde{Y}\\leftarrow \\mathbb{swatmf}(X^*,\\tilde{U}, \\tilde{\\psi}_0 )+E$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e5d8d",
   "metadata": {},
   "source": [
    "# Bayes' Theorem\n",
    "> ### _\"When the facts change, I change my mind. What do you do, sir?\"_\n",
    "> --John Maynard Keynes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e98ba",
   "metadata": {},
   "source": [
    "Loosely, Bayes' Theorem can be interpeted as \n",
    "\n",
    "$$P\\left(\\boldsymbol{\\theta}|\\textbf{d}\\right)=\\frac{P\\left(\\textbf{d}|\\boldsymbol{\\theta}\\right) P\\left (\\boldsymbol{\\theta}\\right)}{P\\left(\\textbf{d}\\right)}$$\n",
    " \n",
    "in which $\\boldsymbol{\\theta}$ are parameters and $\\mathbf{d}$ are the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7ee1e",
   "metadata": {},
   "source": [
    "This is really just rearranging the law of conditional probabilities:\n",
    "\n",
    "$$P\\left(\\boldsymbol{\\theta}|\\textbf{d}\\right)P\\left(\\textbf{d}\\right)=P\\left(\\textbf{d}|\\boldsymbol{\\theta}\\right) P\\left(\\boldsymbol{\\theta}\\right)$$\n",
    "\n",
    "_Um, what?_ Let's use pictures to make this easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52baecf2",
   "metadata": {},
   "source": [
    "## A Venn diagram to explore conditional probabilities\n",
    "<img src=\"imgs/conditional_probability.png\" style=\"inline: left; width: 50%\" >\n",
    "\n",
    "By [Gnathan87 - Own work, CC0](https://commons.wikimedia.org/w/index.php?curid=15991401)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384ba61",
   "metadata": {},
   "source": [
    "What is the probability of $A$ if we know we are in $B_1$? The equation for this is:\n",
    "\n",
    "$$P\\left(A|B_1\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8256e5",
   "metadata": {},
   "source": [
    "It is easy to see that it is 100% or:\n",
    "\n",
    "$$P\\left(A|B_1\\right)=1.0$$\n",
    "\n",
    "Why? Because the $B_1$ circle is entirely within the $A$ circle. Therefore, if we know we are in $B_1$, then we must also be in $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46162d33",
   "metadata": {},
   "source": [
    "As a general rule, we can state \n",
    "$$P\\left(A|B_1\\right)=\\frac{P\\left(A\\cap B_1\\right)}{P\\left(B_1\\right)}$$\n",
    "\n",
    "or, equivalently \n",
    "$$P\\left(A\\cap B_1\\right)=P\\left(A|B_1\\right)P\\left(B_1\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15bfae8",
   "metadata": {},
   "source": [
    "So what about $P\\left(A|B_2\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad524e",
   "metadata": {},
   "source": [
    "$$P\\left(A|B_2\\right)=\\frac{P\\left(A\\cap B_2\\right)}{P\\left(B_2\\right)}=\\frac{0.12}{0.12+0.04}=0.75$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83427121",
   "metadata": {},
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "Now we can derive Bayes' theorem because joint probabilities are symmetrical. Switching notation to \n",
    "$\\boldsymbol{\\theta} \\text{ and }\\mathbf{d}$:\n",
    "\n",
    "$$P\\left(\\boldsymbol{\\theta}\\cap \\mathbf{d}\\right)=P\\left(\\mathbf{d}\\cap \\boldsymbol{\\theta}\\right)$$\n",
    "\n",
    "$$P\\left(\\boldsymbol{\\theta}|\\textbf{d}\\right) P\\left(\\textbf{d}\\right) = P\\left(\\textbf{d}|\\boldsymbol{\\theta}\\right) P\\left(\\boldsymbol{\\theta}\\right)$$\n",
    "\n",
    "With the tiniest little algebra, we get Bayes' theorem -- #boom#!\n",
    "\n",
    "$$P\\left(\\boldsymbol{\\theta}|\\textbf{d}\\right) = \\frac{P\\left(\\textbf{d}|\\boldsymbol{\\theta}\\right) P\\left(\\boldsymbol{\\theta}\\right)}{P\\left(\\textbf{d}\\right)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6073e47",
   "metadata": {},
   "source": [
    "# So, what does this really mean?  \n",
    "\n",
    "## A practical example\n",
    "\n",
    "Let's play with a concrete example, one hinging on life, death, trust, and promises kept!\n",
    "<img src=\"imgs/garden.png\" style=\"inline: left; width: 100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ee52a",
   "metadata": {},
   "source": [
    "My wife and kids garden Korean vegetable sesame leaves, lettuce, chili and watermelon.\n",
    "And they were going away to Korea for two months the last summer. Back then, If the vegetables got watered, their probability of dying was 10%. They are 80% likely to die if they don't get watered. So my wife and kids asked me to water them; they were 75% certain I would do it.\n",
    "\n",
    "We can express this all in terms of probabilities and conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e2957",
   "metadata": {},
   "source": [
    "### First a couple definitions:\n",
    "\n",
    "$\\theta_w$: I water the garden.\n",
    "\n",
    "$\\theta_{nw}$: I forget to water the garden.\n",
    "\n",
    "$d_a$: the garden is alive when they return \n",
    "\n",
    "$d_d$: the vegetables are dead when they return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7921ab",
   "metadata": {},
   "source": [
    "$\\mathbf{d} = [d_a,d_d]$: a vector of all possible outcomes\n",
    " \n",
    "$\\boldsymbol{\\theta} = [\\theta_w,\\theta_{nw}]$: a vector of all possible outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2f7b6",
   "metadata": {},
   "source": [
    "Let's express what we know in probability equations:\n",
    "$$P\\left(d_d | \\theta_w\\right)=0.10$$\n",
    "$$P\\left(d_d | \\theta_{nw}\\right)=0.8$$\n",
    "$$P\\left(\\theta_w\\right)=0.75$$\n",
    "$$P\\left(\\theta_{nw}\\right)=0.25$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa1792",
   "metadata": {},
   "source": [
    "And we can assign these as python variables to get our maths groove on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDd_thw = 0.1\n",
    "PDd_thnw = 0.8\n",
    "Prior_thw = 0.75\n",
    "Prior_thnw = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8bdde",
   "metadata": {},
   "source": [
    "Now we can ask questions like, \"what is the probability the vagetable is dead?\"\n",
    "\n",
    "To calculate, we add up all the conditional probablities like this:\n",
    "\n",
    "$$P\\left(d_d\\right) = P\\left(d_d\\cap\\theta_w\\right) + P\\left(d_d\\cap\\theta_{nw}\\right)$$\n",
    "\n",
    "$$P\\left(d_d\\right) = P\\left(d_d|\\theta_w\\right)P\\left(\\theta_w\\right) + P\\left(d_d|\\theta_{nw}\\right)P\\left(\\theta_{nw}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDd = PDd_thw*Prior_thw + PDd_thnw*Prior_thnw\n",
    "print ('Probability vegetable is dead = {0:.3f}'.format(PDd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394bca0",
   "metadata": {},
   "source": [
    "Since we only have two discrete outcomes, the probability of the vegetable being alive is simply \n",
    "\n",
    "$$P\\left(d_a\\right)=1-P\\left(d_d\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDa = 1-PDd\n",
    "print ('Probability vegetable is alive = {0:.3f}'.format(PDa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747af38",
   "metadata": {},
   "source": [
    "Great! So we can incorporate all the possible arrangements of events to determine likely outcomes. But....what we are _really_ interested in is what we learn with partial information. This is where household harmony can be made or broken!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb26bb3",
   "metadata": {},
   "source": [
    "## Learning from information\n",
    "\n",
    "They come home and see that the vegetable is dead (crumbs!). Who to blame? What is the probability that I forgot to water it? \n",
    "\n",
    "Mathematically, this is;\n",
    "$$P\\left(\\theta_{nw} | d_d\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92340aa",
   "metadata": {},
   "source": [
    "We can use Bayes' theorem to evaluate this new information (e.g. we have observed that the plant is dead)\n",
    "\n",
    "$$P\\left(\\theta_{nw} | d_d\\right) = \\frac{P\\left(d_d | \\theta_{nw}\\right) P\\left(\\theta_{nw}\\right)}{P\\left(d_d\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f59ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PthnwDd = PDd_thnw * Prior_thnw/ PDd\n",
    "print (\"Probability that I failed to water the vegetable\")\n",
    "print(\"having seen it's dead is {0:.3f}\".format(PthnwDd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195198c3",
   "metadata": {},
   "source": [
    "Alternatively, we can see the converse: How likely did our partner water the plant given that it's alive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577c8e9",
   "metadata": {},
   "source": [
    "$$P\\left(\\theta_w | d_a\\right) = \\frac{P\\left(d_a | \\theta_w\\right) P\\left(\\theta_w\\right)}{P\\left(d_a\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67dbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PthwDa = (1-PDd_thw) * Prior_thw/ PDa\n",
    "print (\"Probability that I did water the plant\")\n",
    "print (\"having seen it's alive is {0:.3f}\".format(PthwDa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ae5c9",
   "metadata": {},
   "source": [
    "How to update our prior probability or trust?\n",
    "<img src=\"imgs/prior_confidence.jpg\" style=\"inline: left; width: 50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75486d04",
   "metadata": {},
   "source": [
    "### Continuous variables\n",
    "\n",
    "Right then, but we are in the world of continuous variables, not simple discrete probabilities\n",
    "\n",
    "This means that we end up with probability density functions rather than discrete probabilities and the denominator on the RHS gets tricky to evaluate (the total probability). Luckily, we are mostly conncerned with finding the parameters that maximize the probability and less concerned with the probability itself.\n",
    "\n",
    "<img src=\"imgs/Fig10.3_Bayes_figure.png\" style=\"inline: left; width: 75%; margin-right: 1%; margin-bottom: 0.5em;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccccb2",
   "metadata": {},
   "source": [
    "This is a learning framework, where what we know at the end is a function of what we started with and what we _learned_ through a new experiment (model) or new information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01593330",
   "metadata": {},
   "source": [
    "$$\\underbrace{P(\\boldsymbol{\\theta}|\\textbf{d})}_{\\substack{\\text{posterior} \\\\ \\text{pdf}}} \\propto \\underbrace{\\mathcal{L}( \\boldsymbol{\\theta}| \\textbf{d})}_{\\substack{\\text{likelihood} \\\\ \\text{function}}} \\underbrace{P(\\boldsymbol{\\theta})}_{\\substack{\\text{prior } \\\\ \\text{pdf}}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb3e45",
   "metadata": {},
   "source": [
    "$$\\underbrace{P(\\boldsymbol{\\theta}|\\textbf{d})}_{\\substack{\\text{what we} \\\\ \\text{know now}}} \\propto \\underbrace{\\mathcal{L}(\\boldsymbol{\\theta} | \\textbf{d})}_{\\substack{\\text{what we} \\\\ \\text{learned}}} \\underbrace{P(\\boldsymbol{\\theta})}_{\\substack{\\text{what we} \\\\ \\text{knew}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55be4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayes_helper as bh\n",
    "from ipywidgets import interact\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba99b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh.plot_posterior(prior_mean=10, prior_std=11, likeli_mean = 25, likeli_std=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(bh.plot_posterior,\n",
    "         prior_mean=(1, 20., .5), likeli_mean=(1, 20, 1), \n",
    "         prior_std=(.1, 8, .1), likeli_std=(.1, 8, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb49f78",
   "metadata": {},
   "source": [
    "# The mandatory \"coin-flipping example\" \n",
    "> _Borrowed from **Bayesian Methods for Hackers**. The full Github repository is available [here](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)_\n",
    "\n",
    "We can start with an \"ignorance\" prior - equal probabilities of all outcomes (both, in the case---heads and tails). By flipping a coin we can observe outcomes, constantly updating and learning from each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac62554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.beta\n",
    "max_trials = 1000\n",
    "n_trials = [0, 1, 2, 3, 4, 5, 8, 15, 30, 50, 70, 100, 200, 500, max_trials]\n",
    "data = stats.bernoulli.rvs(0.5, size=n_trials[-1])\n",
    "x = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()\n",
    "def animate(i):\n",
    "    N = n_trials[i]\n",
    "    heads = data[:N].sum()\n",
    "    y = dist.pdf(x, 1 + heads, 1 + N - heads)\n",
    "    ax.plot(x, y, label=\"observe %d tosses,\\n %d heads\" % (N, heads))\n",
    "    ax.fill_between(x, 0, y, color=\"#348ABD\", alpha=0.3)\n",
    "    ax.vlines(0.5, 0, 4, color=\"k\", linestyles=\"--\", lw=1)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.autoscale(tight=True)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[-1:], labels[-1:])\n",
    "    return\n",
    "plt.suptitle(\"Bayesian updating of posterior probabilities\",\n",
    "             y=0.98,\n",
    "             fontsize=14)\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(n_trials), interval=1000)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81254b5d",
   "metadata": {},
   "source": [
    "The posterior probabilities are represented by the curves, and our uncertainty is proportional to the width of the curve. As the plot above shows, as we start to observe data our posterior probabilities start to shift and move around. Eventually, as we observe more and more data (coin-flips), our probabilities will tighten closer and closer around the true value of $p=0.5$ (marked by a dashed line). \n",
    "\n",
    "Notice that the plots are not always *peaked* at 0.5. There is no reason it should be: recall we assumed we did not have a prior opinion of what $p$ is. In fact, if we observe quite extreme data, say 8 flips and only 1 observed heads, our distribution would look very biased *away* from lumping around 0.5 (with no prior opinion, how confident would you feel betting on a fair coin after observing 8 tails and 1 head). As more data accumulate, we would see more and more probability being assigned at $p=0.5$, though never all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9609de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
